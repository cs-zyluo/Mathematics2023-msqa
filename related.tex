\section{Related Work}
\label{sec:related}
This section briefly covers past research on employing Large Language Models (LLMs) like GPT for knowledge enhancement in diverse applications, emphasizing different approaches in extractive Reading Comprehension (RC) tasks.

\subsection{Interaction with LLMs}
Recent research emphasizes the significance of extracting knowledge from Large Language Models (LLMs) in domains characterized by limited knowledge base coverage~\citep{fang2021leveraging}.
GPT3Mix~\citep{yoo2021gpt3mix} extends the utility of LLMs to text data augmentation, thereby elevating the performance of machine learning models. 
AugGPT~\citep{dai2023auggpt} further contributes to natural language processing tasks in scenarios with constrained data by generating new textual data through interactions with ChatGPT.

In the realm of common-sense reasoning, the effectiveness of prompt generation and reinforcement learning has been demonstrated in enhancing question-answering performance ~\citep{liu2021generated,liu2022rainier}. 
LLMs excel in the generation of contextual information ~\citep{yu2022generate}, 
and the collaborative impact of iterative retrieval and generation amplifies overall LLM performance~\citep{shao2023enhancing}.

Within question-answering systems, ~\citep{huang2023enhancing} improve model context learning in multi-span tasks through answer feedback mechanisms. 
Comparative analyses between ChatGPT and traditional knowledge base question-answering models explore its potential as an alternative~\citep{tan2023can}. 
In addressing LLM limitations in handling factual information, ~\citep{ren2023investigating} employ retrieval-enhanced techniques to enhance the model's understanding of fact-based queries.

~\citep{wei2023zero} showcase zero-shot information extraction through ChatGPT interaction, 
while~\citep{wei2023zero} explore enhancing content-based recommendation systems by combining open-source and closed-source language models. 
Collectively, these studies highlight LLM innovation across diverse domains, offering valuable insights for future research.

However, challenges persist, including non-standardized response structures and ambiguity. 
Our research focuses on designing a set of templates for interacting with ChatGPT, encompassing entity interpretation, entity relationship analysis, rewriting, and summary information for each task. 
Additionally, we introduce a knowledge injection method to directly incorporate knowledge generated by large models into training texts, thereby enhancing task performance. 
This approach aims to provide a structured and comprehensive framework for leveraging LLMs in various applications.


\subsection{Neural Models for RC}
Research in reading comprehension grows rapidly, and many successful neural-based RC models have been proposed in this area. 
%As discussed in \secref{sec:related_dataset}, the extractive RC datasets support to cast RC as answer extraction task.
Typically, neural  models~\citep{DBLP:conf/aaai/PangLGXSC19,DBLP:conf/iclr/Wang017a,DBLP:conf/iclr/XiongZS17} for RC are composed of two components, a context encoder and an answer decoder. 
The context encoder is used to encode the information of questions, contexts and their interactions in-between. Then, the answer decoder aims to generate the answer texts based on outputs of the context encoder. 
To make the answer decoder compatible with the answer extraction task, 
Pointer Network~\citep{DBLP:conf/nips/VinyalsFJ15} model has been adopted to
% tackle the problem that 
%generates answer texts from contexts. 
copy tokens from the given contexts as answers~\citep{DBLP:conf/acl/KadlecSBK16,DBLP:conf/emnlp/TrischlerYYBSS16}.
\cite{DBLP:conf/iclr/Wang017a} proposed a boundary model, which utilized Pointer Network to predict the start and end indices for an answer span.
%\ZY{single token - single span - multispan}
%Boundary Model
\cite{DBLP:conf/iclr/SeoKFH17} proposed an alternative way for the implementation of answer decoder, that built neural position classifiers upon the encoder outputs, predicting the start and end indices of the answer span in the context.

% neural layer unifies the components of encoder and decoder together that 
Recently, the RC models upgrade the context encoder using pre-trained language models (PrLMs)~\citep{radford2018improving,kenton2019bert,liu2019roberta,lee2020biobert,gu2021domain} 
%such as GPT~\citep{radford2018improving}, BERT~\citep{kenton2019bert} and RoBERTa~\citep{liu2019roberta}
, benefiting from the invention of Transformer~\citep{vaswani2017attention} blocks.
%Benefiting from nowadays pretrained language models (PrLMs) based on Transformer~\cite{vaswani2017attention} blocks, recent RC models upgrades the context encoder using PrLMs such as BERT. 
%the context encoder and answer decoder can be unified into a standard algorithm that utilize PrLMs such as BERT to encode inputs, then predict the start position and end position for the answer span.
\cite{DBLP:conf/naacl/DevlinCLT19}  proposed a standard extractive model for  single-span RC that utilizes BERT to encode inputs, then builds position classifiers to predict where the answer span starts and ends.
However, the answer decoder, whether implemented with Pointer Network or position classifiers, predicts start and end position independently, thus can not distinguish the different answer spans properly.
%Pang et al. proposed HAS-QA~\cite{} which built the conditional pointer network and aggregators to model multiple answer spans.
~\cite{DBLP:conf/emnlp/ZhuAJ0R20} proposed MultiCo which used a contextualized sentence selection method to capture the relevance among multiple sentence-based answer spans in order to form an answer with multiple sentences.
%However, such single-span extractive model is not suitable for multi-span RC task which can be formulated as multi-span extraction.
These models are not well adapted to multi-span RC which can be formulated as more flexible task of multi-span extraction where each span can be a word, phrase, sentence or any continuous string of text.

%the start position and end position of the answer span. 
%PrLMs unify the context encoder and answer decoder as a standard algorithm, that  
%\subsubsection{Single-span Models}
%~\citep{yoon2022,DBLP:journals/corr/abs-2302-01691} 
Extracting a variable number of spans from an input text can be commonly cast as a sequence tagging problem.
\cite{segal2020simple} proposed using a sequence tagging model for multi-span extraction, which predicts whether each token is part of an answer. \cite{yoon2022} employed a similar sequence tagging approach to address extractive question answering~\citep{DBLP:journals/bmcbi/NaseemDKK22}  in the biomedical domain.
\cite{li2022multispanqa} also adopted the tagging model architecture, integrating two sub-tasks: predicting the number of spans to extract and annotating the answer structure within their proposed dataset to capture global information.
ADRAV~\citep{hu2023biomedical} proposed a dynamic routing and answer voting method to further make full use of the hidden layer knowledge of pre-trained models.

More recently, SpanQualifier~\citep{huang2023qualifier} improves Multi-Span Reading Comprehension (MSRC) by explicitly representing spans and modeling interactions within and between them.
Iterative Extractor~\citep{zhang2023many} introduces a classification method for MSRC instances, prompting exploration of strategies to maximize different paradigms' advantages in capturing key information and modeling relationships between questions and context.
While these methods leverage powerful context encoders (PrLMs) with good multi-span answer extraction performance, they fall short in fully harnessing external knowledge, limiting their text comprehension abilities.
LIQUID~\citep{lee2023liquid} was designed to automatically generate list-style QA pairs from unlabeled corpora, using named entities from summarized text as candidate answers and incorporating synthetic data in the tagging model. However, its focus on list QA data narrows the scope of knowledge and lacks specificity, reducing efficiency.
In contrast, AUG significantly advances the field by efficiently enhancing model training. It achieves this by leveraging interactions with a large language model to acquire contextually relevant knowledge directly tied to the data, which can be seamlessly incorporated into the finetuning.

