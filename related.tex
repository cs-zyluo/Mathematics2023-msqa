<<<<<<< HEAD
<<<<<<< HEAD

\section{Related Work}
\label{sec:related}
In this section, we briefly summarize previous work on how Large Language Models (LLMs) such as the GPT series can be utilized as sources of knowledge and content generators to enhance and enrich text data in various application scenarios.
\subsection{Interaction with LLMs}
Large Language Models (LLMs) are powerful natural language processing models in the field of deep learning, 
characterized by their massive parameter scale and outstanding capabilities in understanding and generating natural language text. 
Recent research has highlighted the extraction of relevant knowledge from LLMs, especially in domains lacking proper coverage in knowledge bases~\citep{fang2021leveraging}. 
The application of large language models has expanded from the field of knowledge extraction to text data augmentation. 
GPT3Mix~\citep{yoo2021gpt3mix} has proposed a method that utilizes large language models to generate new text samples, thereby enhancing the performance of machine learning models.
AugGPT~\citep{dai2023auggpt} enhances natural language processing tasks in situations with limited data by interacting with ChatGPT to generate new textual data, further advancing text data augmentation.
In the realm of common sense reasoning,~\citep{liu2021generated,liu2022rainier} suggests that performance in common sense question answering can be effectively improved through prompt generation and reinforcement learning. 
Additionally, LLMs excel in generating contextual information, demonstrating the ability to create rich, 
relevant contexts based on given inputs, as validated in studies such as those by~\citep{yu2022generate}. 
The collaborative effect of iterative retrieval and generation further boosts the performance of LLMs~\citep{shao2023enhancing}. 
However, a limitation in current research is the insufficient interpretation of entities.Our study addresses this gap by interacting with ChatGPT, 
leveraging its powerful generative capabilities to design a series of prompt templates for obtaining more comprehensive entity interpretations.


In the research domain of question-answering systems, ~\citep{huang2023enhancing} improves the context learning ability of models in multi-span question-answering tasks through the introduction of answer feedback mechanisms. 
Comparisons between ChatGPT and traditional knowledge base question-answering models explore its potential as an alternative to traditional KBQA tasks~\citep{tan2023can}, 
investigating differences in providing single versus multiple answers in various contexts within reading comprehension tasks~\citep{zhang2023many}. 
Considering the limitations of LLMs in handling factual information, ~\citep{ren2023investigating} utilizes retrieval-enhanced techniques to enhance the model's understanding and response capabilities to fact-based queries.
Finally, ~\citep{wei2023zero} showcase a new approach to zero-shot information extraction through interaction with ChatGPT, 
while~\citep{liu2023once} explore the enhancement of content-based recommendation systems by combining open-source and closed-source language models. 
These studies not only demonstrate the innovation and advantages of LLMs across various domains but also provide profound insights into future research directions. 
However, the response structures generated by LLMs in these studies are not standardized. 
To address this, our prompt template design aims to ensure that the responses from ChatGPT are easily parsed and processed, enhancing the structured nature of the responses. 
Additionally, dealing with ambiguity and ambiguity is another challenge. 
Through the integration of LLM knowledge sources, our framework aims to provide clearer, more detailed information to better address semantic complexities.


\subsection{Neural Models for RC}
Research in reading comprehension grows rapidly, and many successful neural-based RC models have been proposed in this area. 
%As discussed in \secref{sec:related_dataset}, the extractive RC datasets support to cast RC as answer extraction task.
Typically, neural  models~\citep{DBLP:conf/aaai/PangLGXSC19,DBLP:conf/iclr/Wang017a,DBLP:conf/iclr/XiongZS17} for RC are composed of two components, a context encoder and an answer decoder. 
The context encoder is used to encode the information of questions, contexts and their interactions in-between. Then, the answer decoder aims to generate the answer texts based on outputs of the context encoder. 
To make the answer decoder compatible with the answer extraction task, 
Pointer Network~\citep{DBLP:conf/nips/VinyalsFJ15} model has been adopted to
% tackle the problem that 
%generates answer texts from contexts. 
copy tokens from the given contexts as answers~\citep{DBLP:conf/acl/KadlecSBK16,DBLP:conf/emnlp/TrischlerYYBSS16}.
\cite{DBLP:conf/iclr/Wang017a} proposed a boundary model, which utilized Pointer Network to predict the start and end indices for an answer span.
%\ZY{single token - single span - multispan}
%Boundary Model
\cite{DBLP:conf/iclr/SeoKFH17} proposed an alternative way for the implementation of answer decoder, that built neural position classifiers upon the encoder outputs, predicting the start and end indices of the answer span in the context.

% neural layer unifies the components of encoder and decoder together that 
Recently, the RC models upgrade the context encoder using pre-trained language models (PrLMs)~\citep{radford2018improving,kenton2019bert,liu2019roberta,lee2020biobert,gu2021domain} 
%such as GPT~\citep{radford2018improving}, BERT~\citep{kenton2019bert} and RoBERTa~\citep{liu2019roberta}
, benefiting from the invention of Transformer~\citep{vaswani2017attention} blocks.
%Benefiting from nowadays pretrained language models (PrLMs) based on Transformer~\cite{vaswani2017attention} blocks, recent RC models upgrades the context encoder using PrLMs such as BERT. 
%the context encoder and answer decoder can be unified into a standard algorithm that utilize PrLMs such as BERT to encode inputs, then predict the start position and end position for the answer span.
\cite{DBLP:conf/naacl/DevlinCLT19}  proposed a standard extractive model for  single-span RC that utilizes BERT to encode inputs, then builds position classifiers to predict where the answer span starts and ends.
However, the answer decoder, whether implemented with Pointer Network or position classifiers, predicts start and end position independently, thus can not distinguish the different answer spans properly.
%Pang et al. proposed HAS-QA~\cite{} which built the conditional pointer network and aggregators to model multiple answer spans.
~\cite{DBLP:conf/emnlp/ZhuAJ0R20} proposed MultiCo which used a contextualized sentence selection method to capture the relevance among multiple sentence-based answer spans in order to form an answer with multiple sentences.
%However, such single-span extractive model is not suitable for multi-span RC task which can be formulated as multi-span extraction.
These models are not well adapted to multi-span RC which can be formulated as more flexible task of multi-span extraction where each span can be a word, phrase, sentence or any continuous string of text.

%the start position and end position of the answer span. 
%PrLMs unify the context encoder and answer decoder as a standard algorithm, that  
%\subsubsection{Single-span Models}
%~\citep{yoon2022,DBLP:journals/corr/abs-2302-01691} 
Extracting a variable number of spans from an input text can be commonly cast as a sequence tagging problem.
\cite{segal2020simple} proposed using a sequence tagging model for multi-span extraction, which predicts whether each token is part of an answer. \cite{yoon2022} employed a similar sequence tagging approach to address extractive question answering~\citep{DBLP:journals/bmcbi/NaseemDKK22}  in the biomedical domain.
\cite{li2022multispanqa} also adopted the tagging model architecture, integrating two sub-tasks: predicting the number of spans to extract and annotating the answer structure within their proposed dataset to capture global information.
ADRAV~\citep{hu2023biomedical} proposed a dynamic routing and answer voting method to further make full use of the hidden layer knowledge of pre-trained models.

More recently, SpanQualifier~\cite{huang2023qualifier} improves Multi-Span Reading Comprehension (MSRC) by explicitly representing spans and modeling interactions within and between them.
Iterative Extractor~\cite{zhang2023many} introduces a classification method for MSRC instances, prompting exploration of strategies to maximize different paradigms' advantages in capturing key information and modeling relationships between questions and context.
While these methods leverage powerful context encoders (PrLMs) with good multi-span answer extraction performance, they fall short in fully harnessing external knowledge, limiting their text comprehension abilities.
LIQUID~\cite{lee2023liquid} was designed to automatically generate list-style QA pairs from unlabeled corpora, using named entities from summarized text as candidate answers and incorporating synthetic data in the tagging model. However, its focus on list QA data narrows the scope of knowledge and lacks specificity, reducing efficiency.
In contrast, AUG significantly advances the field by efficiently enhancing model training. It achieves this by leveraging interactions with a large language model to acquire contextually relevant knowledge directly tied to the data, which can be seamlessly incorporated into the finetuning.


\subsection{Interaction with LLMs}
=======
<<<<<<< HEAD

\section{Related Work}
\label{sec:related}
In this section, we briefly summarize previous work on how Large Language Models (LLMs) such as the GPT series can be utilized as sources of knowledge and content generators to enhance and enrich text data in various application scenarios.
\subsection{Interaction with LLMs}
Large Language Models (LLMs) are powerful natural language processing models in the field of deep learning, 
characterized by their massive parameter scale and outstanding capabilities in understanding and generating natural language text. 
Recent research has highlighted the extraction of relevant knowledge from LLMs, especially in domains lacking proper coverage in knowledge bases~\citep{fang2021leveraging}. 
The application of large language models has expanded from the field of knowledge extraction to text data augmentation. 
GPT3Mix~\citep{yoo2021gpt3mix} has proposed a method that utilizes large language models to generate new text samples, thereby enhancing the performance of machine learning models.
AugGPT~\citep{dai2023auggpt} enhances natural language processing tasks in situations with limited data by interacting with ChatGPT to generate new textual data, further advancing text data augmentation.
In the realm of common sense reasoning,~\citep{liu2021generated,liu2022rainier} suggests that performance in common sense question answering can be effectively improved through prompt generation and reinforcement learning. 
Additionally, LLMs excel in generating contextual information, demonstrating the ability to create rich, 
relevant contexts based on given inputs, as validated in studies such as those by~\citep{yu2022generate}. 
The collaborative effect of iterative retrieval and generation further boosts the performance of LLMs~\citep{shao2023enhancing}. 
However, a limitation in current research is the insufficient interpretation of entities.Our study addresses this gap by interacting with ChatGPT, 
leveraging its powerful generative capabilities to design a series of prompt templates for obtaining more comprehensive entity interpretations.


In the research domain of question-answering systems, ~\citep{huang2023enhancing} improves the context learning ability of models in multi-span question-answering tasks through the introduction of answer feedback mechanisms. 
Comparisons between ChatGPT and traditional knowledge base question-answering models explore its potential as an alternative to traditional KBQA tasks~\citep{tan2023can}, 
investigating differences in providing single versus multiple answers in various contexts within reading comprehension tasks~\citep{zhang2023many}. 
Considering the limitations of LLMs in handling factual information, ~\citep{ren2023investigating} utilizes retrieval-enhanced techniques to enhance the model's understanding and response capabilities to fact-based queries.
Finally, ~\citep{wei2023zero} showcase a new approach to zero-shot information extraction through interaction with ChatGPT, 
while~\citep{liu2023once} explore the enhancement of content-based recommendation systems by combining open-source and closed-source language models. 
These studies not only demonstrate the innovation and advantages of LLMs across various domains but also provide profound insights into future research directions. 
However, the response structures generated by LLMs in these studies are not standardized. 
To address this, our prompt template design aims to ensure that the responses from ChatGPT are easily parsed and processed, enhancing the structured nature of the responses. 
Additionally, dealing with ambiguity and ambiguity is another challenge. 
Through the integration of LLM knowledge sources, our framework aims to provide clearer, more detailed information to better address semantic complexities.


\subsection{Neural Models for RC}
Research in reading comprehension grows rapidly, and many successful neural-based RC models have been proposed in this area. 
%As discussed in \secref{sec:related_dataset}, the extractive RC datasets support to cast RC as answer extraction task.
Typically, neural  models~\citep{DBLP:conf/aaai/PangLGXSC19,DBLP:conf/iclr/Wang017a,DBLP:conf/iclr/XiongZS17} for RC are composed of two components, a context encoder and an answer decoder. 
The context encoder is used to encode the information of questions, contexts and their interactions in-between. Then, the answer decoder aims to generate the answer texts based on outputs of the context encoder. 
To make the answer decoder compatible with the answer extraction task, 
Pointer Network~\citep{DBLP:conf/nips/VinyalsFJ15} model has been adopted to
% tackle the problem that 
%generates answer texts from contexts. 
copy tokens from the given contexts as answers~\citep{DBLP:conf/acl/KadlecSBK16,DBLP:conf/emnlp/TrischlerYYBSS16}.
\cite{DBLP:conf/iclr/Wang017a} proposed a boundary model, which utilized Pointer Network to predict the start and end indices for an answer span.
%\ZY{single token - single span - multispan}
%Boundary Model
\cite{DBLP:conf/iclr/SeoKFH17} proposed an alternative way for the implementation of answer decoder, that built neural position classifiers upon the encoder outputs, predicting the start and end indices of the answer span in the context.

% neural layer unifies the components of encoder and decoder together that 
Recently, the RC models upgrade the context encoder using pre-trained language models (PrLMs)~\citep{radford2018improving,kenton2019bert,liu2019roberta,lee2020biobert,gu2021domain} 
%such as GPT~\citep{radford2018improving}, BERT~\citep{kenton2019bert} and RoBERTa~\citep{liu2019roberta}
, benefiting from the invention of Transformer~\citep{vaswani2017attention} blocks.
%Benefiting from nowadays pretrained language models (PrLMs) based on Transformer~\cite{vaswani2017attention} blocks, recent RC models upgrades the context encoder using PrLMs such as BERT. 
%the context encoder and answer decoder can be unified into a standard algorithm that utilize PrLMs such as BERT to encode inputs, then predict the start position and end position for the answer span.
\cite{DBLP:conf/naacl/DevlinCLT19}  proposed a standard extractive model for  single-span RC that utilizes BERT to encode inputs, then builds position classifiers to predict where the answer span starts and ends.
However, the answer decoder, whether implemented with Pointer Network or position classifiers, predicts start and end position independently, thus can not distinguish the different answer spans properly.
%Pang et al. proposed HAS-QA~\cite{} which built the conditional pointer network and aggregators to model multiple answer spans.
~\cite{DBLP:conf/emnlp/ZhuAJ0R20} proposed MultiCo which used a contextualized sentence selection method to capture the relevance among multiple sentence-based answer spans in order to form an answer with multiple sentences.
%However, such single-span extractive model is not suitable for multi-span RC task which can be formulated as multi-span extraction.
These models are not well adapted to multi-span RC which can be formulated as more flexible task of multi-span extraction where each span can be a word, phrase, sentence or any continuous string of text.

%the start position and end position of the answer span. 
%PrLMs unify the context encoder and answer decoder as a standard algorithm, that  
%\subsubsection{Single-span Models}
%~\citep{yoon2022,DBLP:journals/corr/abs-2302-01691} 
Extracting a variable number of spans from an input text can be commonly cast as a sequence tagging problem.
\cite{segal2020simple} proposed using a sequence tagging model for multi-span extraction, which predicts whether each token is part of an answer. \cite{yoon2022} employed a similar sequence tagging approach to address extractive question answering~\citep{DBLP:journals/bmcbi/NaseemDKK22}  in the biomedical domain.
\cite{li2022multispanqa} also adopted the tagging model architecture, integrating two sub-tasks: predicting the number of spans to extract and annotating the answer structure within their proposed dataset to capture global information.
ADRAV~\citep{hu2023biomedical} proposed a dynamic routing and answer voting method to further make full use of the hidden layer knowledge of pre-trained models.

More recently, SpanQualifier~\cite{huang2023qualifier} improves Multi-Span Reading Comprehension (MSRC) by explicitly representing spans and modeling interactions within and between them.
Iterative Extractor~\cite{zhang2023many} introduces a classification method for MSRC instances, prompting exploration of strategies to maximize different paradigms' advantages in capturing key information and modeling relationships between questions and context.
While these methods leverage powerful context encoders (PrLMs) with good multi-span answer extraction performance, they fall short in fully harnessing external knowledge, limiting their text comprehension abilities.
LIQUID~\cite{lee2023liquid} was designed to automatically generate list-style QA pairs from unlabeled corpora, using named entities from summarized text as candidate answers and incorporating synthetic data in the tagging model. However, its focus on list QA data narrows the scope of knowledge and lacks specificity, reducing efficiency.
In contrast, AUG significantly advances the field by efficiently enhancing model training. It achieves this by leveraging interactions with a large language model to acquire contextually relevant knowledge directly tied to the data, which can be seamlessly incorporated into the finetuning.


\subsection{Interaction with LLMs}
>>>>>>> 14af3a787717720ea39cd58099013268fd5004a2
