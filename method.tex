\section{Approach}
\subsection{our framework}
\begin{figure*}[h]
	\centering
	\includegraphics[width=18cm]{overview.png}
	\caption{An overview of our automatic information augmentation framework. \textbf{(a) Step 1}: Interact with ChatGPT to Get Auxiliary Information. \textbf{(b) Step 2}: Distilling the Information and Injecting Them into Context \textbf{(c) Step 3}: Input the Augmented Context into Tagging Model}
	\label{fig:overview}
\end{figure*}   

 Given a question $Q_i \in Q = \{Q_1,...,Q_n\}$ and a context $C_i \in C = \{C_0,...,C_n\}$, where $C_i$ contains m tokens $c_0,..,c_m$, the objective of multi-span question answering is to identify a set of answer spans $A_i = \{a_0,...,a_s\}$ within the context. Here, each answer span $a_j \in A_i$ is represented as $a_j = c_{s_l},...,c_{e_l}$, where $s_l$ and $e_l$ denote the start and end positions of the l-th answer span, respectively.
 Following the observation of~\cite{li2022multispanqa}, We adopt the BIO tagging scheme to mark answer spans in the context where words are tagged as either part of the answer(\textbf{B}egin, \textbf{I}nside) or not (\textbf{O}ther). Formally, BIO tagging scheme is represented by a tag set $\tau = \{B, I, O\}$.

 Intuitively, large language models (LLMs) can serve as supplementary sources of external knowledge, compensating for the restricted semantic comprehension and limited information perception inherent in pre-trained language models). 
 We then propose a novel \textbf{G}enerative \textbf{I}nformation \textbf{A}ugme\textbf{NT}ation framework (GIANT) for multi-span question answering.
 GIANT employs a plug-and-play strategy to integrate the knowledge from large language models into the input layer of tagging models, built upon pre-trained language models.
 
 
 The overview of GIANT is depicted as \figref{fig:overview}.
 This process comprises the following steps:
 \textbf{1)Prompting}: constructing instruction templates to leverage language models for generating diversified data, involving entity elucidation, entity relationships, content continuation, and summarization.
 \textbf{2)Generating}: the large language model generating new knowledge based on the designed prompts.
 \textbf{3)Updating}: filtering the generated data, and combine it with metadata in different forms.
 \textbf{4)Training}: employing the knowledge-enhanced data to train a tagging model.

 In the following section, we will explore how GIANT utilizes a large language model to generate knowledge. Subsequently, we will delve into the methods employed by GIANT to filter these knowledge, amalgamate them into meta-context, and orchestrate ensemble strategies to leverage these knowledge effectively.

\textcolor{red}{提示的模板放在哪里描述比较好？}
\label{sec:prompt_construction}
\begin{figure*}[h]
	\centering
	\includegraphics[width=14.5cm]{Prompts_construction.png}
	\caption{Prompt Templates and it's Construction}
	\label{fig:prompt_template}
\end{figure*}   

\subsection{LLMs as Knowledge Source}
 GIANT leverages a large language model as an external knowledge source, utilizing it to generate augmented data $K_i = \{E_i, S_i, R_i, F_i\}$ from multiple perspectives. Here, $E_i$ represents named entity elucidation, $R_i$ denotes entity relations, $S_i$ pertains to content summarization, and $F_i$ encompasses content continuation.

 Within these knowledge perspectives, named entity elucidation and entity relations provide factual knowledge, with GIANT synthesizing knowledge cues for the model by jointly injecting them into metadata.
 Summarization acts as a mechanism for information filtering, sieving and retaining entity interpretations, thereby ensuring augmentation efficiency. 
 Content continuation introduces relevant external knowledge by extending contextual content.
 
\subsubsection{LLMs as Content Summerizer}
 One of the pivotal factors in entity knowledge selection lies in ensuring that these entities exhibit semantic relevance within the specified context. The incorporation of disparate entity explanations and relationship analyses at the model's input layer may lead to the valueless augmentation of contexts, consequently impeding the model's learning.
 
 Utilizing the potent capabilities of large language models, we employ them to succinctly summarize the context $C_i$ into $S_i$, followed by extracting entities-based factual knowledge from $S_i$.
 By considering summarization as a knowledge filter, we can achieve information augmentation with greater efficiency.
 
\subsubsection{LLMs as Named Entity Elucidator}
 The resolution of multi-span questions usually entails identifying named entities. This intricate task can be greatly improved by tapping into the specialized knowledge of named entities. This expertise directly aids the model in comprehending rare lexical items within its pre-training corpus and adapting to context-specific terminologies during fine-tuning. 
  
 While previous research has delved into entity knowledge either through training or direct application of fine-tuned NER models like spaCy and BERN2, these models have limitations stemming from their narrow training datasets and their tendency to solely provide extracted entities without contextualized meanings due to their inflexible design.  
 In comparison, generative large language models, endowed with advanced representation capabilities and abundant training data, present a promising avenue for achieving more precise and comprehensive entity delineations.
  
 GIANT utilizes a large language model to generate entity elucidation $E_i = \{e_0,...,e_h\}$ for each context $C_i$, where $i=1,..,n$. 
 Following this, it adopts a hybrid methodology integrating both regular expressions and semantic dependency analysis models, exemplified by spaCy, to parse the produced text.
 This systematic procedure culminates in the establishment of a ``Entity-Elucidation'' knowledge base oriented towards the elucidation of entities. 
 Furthermore, as \figref{fig:entity_insertion} presented, we seamlessly integrates the retrieved entity $e_t \in E_i$ mentioned within context $C_i$ by inserting its corresponding explanation immediately after the entity mention, significantly enriching the original text content while ensuring coherence and clarity are upheld.
 
\label{sec:LLMs as Named Entity Elucidator}
\begin{figure*}[h]
	\centering
	\includegraphics[width=10cm]{EntityInsert.png}
	\caption{The Process of Inserting Entity Explanation into Context}
	\label{fig:entity_insertion}
\end{figure*}

\begin{figure*}[h]
	\centering
	\includegraphics[width=10cm]{RandomConcat.png}
	\caption{The Process of Concatenation of Original Context and Auxiliary Information}
	\label{fig:random_concatenate}
\end{figure*}  

\subsubsection{Information Integration}
\label{sec:information_integration}
In addition to the aforementioned three types of information, we seek more valuable enhanced information by requesting summaries for each question-context pair from Large Language Models (LLMs). We then extract entities mentioned in the summaries to capture those highly relevant to the core context. Moreover, recognizing the intrinsic connection between entity explanation and entity relationship parsing, we categorize them as complementary information for context enrichment, with context continuation serving as supplementary information. This approach aims to augment the model's internal text-parsing knowledge with external world knowledge through enhanced information. 

To further investigate the efficacy of multi-information integration, leveraging enhanced information derived from Large Language Models (LLMs), we employ two distinct approaches to combine entity knowledge and context continuity. The first approach involves the incorporation of all information within the original context, with each type of information undergoing individualized knowledge injection. The second approach, referred to as "bagging," treats models trained on diverse information sources as voters. For each query, we employ all model predictions for token-level BIO labeling within the context, determining the final prediction through a majority vote. In cases of tie-breaking, we prioritize the logits' magnitude to ascertain the ultimate prediction.

