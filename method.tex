\section{Approach}
\subsection{our framework}
\begin{figure*}[h]
	\centering
	\includegraphics[width=18cm]{overview.png}
	\caption{An overview of our automatic information augmentation framework. \textbf{(a) Step 1}: Interact with ChatGPT to Get Auxiliary Information. \textbf{(b) Step 2}: Distilling the Information and Injecting Them into Context \textbf{(c) Step 3}: Input the Augmented Context into Tagging Model}
	\label{fig:overview}
\end{figure*}   

 Given a question $Q_i \in Q = \{Q_1,...,Q_n\}$ and a context $C_i \in C = \{c_0,...,c_n\}$, where $C_i$ contains m tokens $c_0,..,c_m$, the objective of multi-span question answering is to identify a set of answer spans $A_i = \{a_0,...,a_s\}$ within the context. Here, each answer span $a_j \in A_i$ is represented as $a_j = c_{s_l},...,c_{e_l}$, where $s_l$ and $e_l$ denote the start and end positions of the l-th answer span, respectively.
 Following the observation of~\cite{li2022multispanqa}, We adopt the BIO tagging scheme to mark answer spans in the context where words are tagged as either part of the answer(\textbf{B}egin, \textbf{I}nside) or not (\textbf{O}ther). Formally, BIO tagging scheme is represented by a tag set $\tau = \{B, I, O\}$.

 Intuitively, large language models (LLMs) can serve as supplementary sources of external knowledge, compensating for the restricted semantic comprehension and limited information perception inherent in pre-trained language models). 
 We then propose a novel \textbf{G}enerative \textbf{I}nformation \textbf{A}ugme\textbf{NT}ation framework (GIANT) for multi-span question answering.
 GIANT employs a plug-and-play strategy to integrate the knowledge from large language models into the input layer of tagging models, built upon pre-trained language models.
 
 
 The overview of GIANT is depicted as \figref{fig:overview}.
 This process comprises the following steps:
 \textbf{1)Prompting}: constructing instruction templates to leverage language models for generating diversified data, involving entity elucidation, entity relationships, content continuation, and summarization.
 \textbf{2)Generating}: the large language model generating new knowledge based on the designed prompts.
 \textbf{3)Updating}: filtering the generated data, and combine it with metadata in different forms.
 \textbf{4)Training}: employing the knowledge-enhanced data to train a tagging model.
 

 Consider $Gen(\cdot)$ as a generative large language model, and let $Inj(\cdot)$ function as a mapping that takes the generated content $K_i$ and context $C_i$, producing the augmented context $\tilde{C_i}$. $P(\cdot)$ represents a mapping of the context $C$ onto instantiated instructions, while $Tag(\cdot)$ serves as a QA model, and $Filter(\cdot)$ denotes the sieving operation.
 With these definitions, GIANT can be formulated as follows:
 \begin{align*}
 	K_i &= Gen(C_i,P(C_i)) \tag{1}\\
 	\tilde{C_i} &= Inj(K_i,C_i) \tag{2}\\
 	A_i &= Tag(Q_i,~Filter(\tilde{C_i})) \tag{3}
 \end{align*}
 
 
 In the following section, we will explore how GIANT utilizes a large language model to generate knowledge. Subsequently, we will delve into the methods employed by GIANT to filter these knowledge, amalgamate them into meta-context, and orchestrate ensemble strategies to leverage these knowledge effectively.

\textcolor{red}{提示的模板放在哪里描述比较好？}
\label{sec:prompt_construction}
\begin{figure*}[h]
	\centering
	\includegraphics[width=14.5cm]{Prompts_construction.png}
	\caption{Prompt Templates and it's Construction}
	\label{fig:prompt_template}
\end{figure*}   

\subsection{LLMs as Knowledge Source}
 GIANT employs a large language model to generate augmented data from multiple perspectives, including named entity elucidation, entity relations, content summarization, and content continuation.
 The perspectives of named entity elucidation and entity relations provide factual knowledge, with GIANT forming knowledge cues for the model by jointly injecting them into metadata. 
 Summarization acts as a mechanism for information filtering, sieving and retaining entity interpretations to ensure augmentation efficiency. 
 Content continuation introduces relevant external knowledge by extending contextual content.


\subsubsection{LLMs as Named Entity Elucidator}

  The knowledge of named entity elucidation serves as a direct aid to the model in comprehending infrequent lexical items within its pre-training corpus, context-specific terminologies in fine-tuning data, and ambiguous conceptual nuances in real world. 
  Meanwhile, the inherent robustness of large language models, owing to their sophisticated language representation capabilities and vast training corpora, facilitates the provision of more precise and comprehensive entity delineations. 
  Therefore, GIANT utilizes large language models to generate entity definitions for each question-answer pair, employing a combined approach of regular expressions and semantic dependency analysis models (Spacy) to parse the generated text and then constructs a branch-oriented "entity-elucidation" knowledge base.
  Ultimately, the obtained knowledge base is used to retrieve entity elucidations and insert them behind entities in the original context, enriching the original textual content.
 
\label{sec:information_injection}
\begin{figure*}[h]
	\centering
	\includegraphics[width=10cm]{EntityInsert.png}
	\caption{The Process of Inserting Entity Explanation into Context}
	\label{fig:entity_insertion}
\end{figure*}


\begin{figure*}[h]
	\centering
	\includegraphics[width=10cm]{RandomConcat.png}
	\caption{The Process of Concatenation of Original Context and Auxiliary Information}
	\label{fig:random_concatenate}
\end{figure*}  

\subsection{Information Integration}
\label{sec:information_integration}
In addition to the aforementioned three types of information, we seek more valuable enhanced information by requesting summaries for each question-context pair from Large Language Models (LLMs). We then extract entities mentioned in the summaries to capture those highly relevant to the core context. Moreover, recognizing the intrinsic connection between entity explanation and entity relationship parsing, we categorize them as complementary information for context enrichment, with context continuation serving as supplementary information. This approach aims to augment the model's internal text-parsing knowledge with external world knowledge through enhanced information. 

To further investigate the efficacy of multi-information integration, leveraging enhanced information derived from Large Language Models (LLMs), we employ two distinct approaches to combine entity knowledge and context continuity. The first approach involves the incorporation of all information within the original context, with each type of information undergoing individualized knowledge injection. The second approach, referred to as "bagging," treats models trained on diverse information sources as voters. For each query, we employ all model predictions for token-level BIO labeling within the context, determining the final prediction through a majority vote. In cases of tie-breaking, we prioritize the logits' magnitude to ascertain the ultimate prediction.

